#!/usr/bin/env python3
"""
Advanced RuneLite Vision System - Cutting Edge Computer Vision
Uses machine learning, template matching, edge detection, and color clustering
"""

import cv2
import numpy as np
import time
from typing import Dict, List, Tuple, Any, Optional
from enum import Enum
import logging

# Try to import advanced ML libraries
try:
    from sklearn.cluster import KMeans
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

try:
    from scipy import ndimage
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False

logger = logging.getLogger(__name__)

class RuneLiteTab(Enum):
    COMBAT = "combat"
    STATS = "stats" 
    QUEST = "quest"
    INVENTORY = "inventory"
    EQUIPMENT = "equipment"
    PRAYER = "prayer"
    MAGIC = "magic"
    CLAN = "clan"
    UNKNOWN = "unknown"

class AdvancedRuneLiteVision:
    """
    Cutting-edge computer vision system for RuneLite
    """
    
    def __init__(self):
        self.analysis_times = []
        logger.info("ðŸš€ Advanced RuneLite Vision System initialized")
        logger.info(f"ðŸ“š ML Libraries: sklearn={HAS_SKLEARN}, scipy={HAS_SCIPY}")
    
    def analyze_runelite_advanced(self, screenshot: np.ndarray) -> Dict[str, Any]:
        """
        CUTTING-EDGE analysis using multiple CV techniques
        """
        start_time = time.time()
        
        try:
            h, w = screenshot.shape[:2]
            logger.debug(f"ðŸ”¬ Advanced CV analysis: {w}x{h}")
            
            results = {
                'active_tab': RuneLiteTab.UNKNOWN,
                'orbs': {},
                'inventory_items': [],
                'chat_messages': [],
                'detection_method': 'advanced_cv',
                'confidence_scores': {}
            }
            
            # ðŸŽ¯ Method 1: Advanced Tab Detection
            tab_result = self._detect_tabs_with_template_matching(screenshot)
            results['active_tab'] = tab_result['tab']
            results['confidence_scores']['tab_detection'] = tab_result['confidence']
            
            # ðŸ”® Method 2: ML-based Orb Detection
            orbs_result = self._detect_orbs_with_ml(screenshot)
            results['orbs'] = orbs_result['orbs']
            results['confidence_scores']['orb_detection'] = orbs_result['confidence']
            
            # ðŸ“¦ Method 3: Advanced Inventory Analysis
            if tab_result['tab'] == RuneLiteTab.INVENTORY:
                inv_result = self._detect_inventory_with_contours(screenshot)
                results['inventory_items'] = inv_result['items']
                results['confidence_scores']['inventory'] = inv_result['confidence']
            
            # ðŸ’¬ Method 4: Smart Chat Detection
            chat_result = self._detect_chat_with_text_analysis(screenshot)
            results['chat_messages'] = chat_result['messages']
            results['confidence_scores']['chat'] = chat_result['confidence']
            
            # Performance tracking
            processing_time = time.time() - start_time
            self.analysis_times.append(processing_time)
            
            results['performance'] = {
                'processing_time': processing_time,
                'method': 'advanced_ml_cv',
                'total_confidence': np.mean(list(results['confidence_scores'].values()))
            }
            
            logger.debug(f"ðŸŽ‰ Advanced analysis: {processing_time:.3f}s")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Advanced analysis failed: {e}")
            return self._empty_results()
    
    def _detect_tabs_with_template_matching(self, screenshot: np.ndarray) -> Dict[str, Any]:
        """
        ðŸŽ¯ ADVANCED TAB DETECTION using multiple CV techniques
        """
        h, w = screenshot.shape[:2]
        
        # Focus on interface area (right side of RuneLite)
        interface_start = int(w * 0.82)  # Right 18% of screen
        interface_region = screenshot[:, interface_start:]
        
        if interface_region.size == 0:
            return {'tab': RuneLiteTab.UNKNOWN, 'confidence': 0.0}
        
        # ðŸ”¥ METHOD 1: Edge-based tab detection
        gray_interface = cv2.cvtColor(interface_region, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray_interface, 30, 100)
        
        # ðŸ”¥ METHOD 2: Color-based selection detection
        hsv_interface = cv2.cvtColor(interface_region, cv2.COLOR_BGR2HSV)
        
        # Focus on bottom area where OSRS tabs are located
        tab_area_start = int(interface_region.shape[0] * 0.75)  # Bottom 25%
        tab_region = interface_region[tab_area_start:, :]
        
        if tab_region.size == 0:
            return {'tab': RuneLiteTab.UNKNOWN, 'confidence': 0.0}
        
        # ðŸ§  ADVANCED: Detect bright/selected areas
        gray_tabs = cv2.cvtColor(tab_region, cv2.COLOR_BGR2GRAY)
        
        # Use multiple thresholding techniques
        _, thresh1 = cv2.threshold(gray_tabs, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        thresh2 = cv2.adaptiveThreshold(gray_tabs, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
        
        # Combine thresholds
        combined_thresh = cv2.bitwise_or(thresh1, thresh2)
        
        # Find contours (potential tabs)
        contours, _ = cv2.findContours(combined_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        best_tab = RuneLiteTab.UNKNOWN
        best_confidence = 0.0
        
        for contour in contours:
            area = cv2.contourArea(contour)
            
            # Filter for tab-sized areas
            if 100 < area < 2000:
                # Get bounding box
                x, y, w, h = cv2.boundingRect(contour)
                
                # Calculate position in tab strip
                tab_center_x = x + w // 2
                tab_strip_width = tab_region.shape[1]
                
                # Map position to tab type (OSRS has ~8 tabs horizontally)
                tab_position = tab_center_x / tab_strip_width
                
                # Map position to specific tab
                if 0.0 <= tab_position < 0.125:
                    candidate_tab = RuneLiteTab.COMBAT
                elif 0.125 <= tab_position < 0.25:
                    candidate_tab = RuneLiteTab.STATS
                elif 0.25 <= tab_position < 0.375:
                    candidate_tab = RuneLiteTab.QUEST
                elif 0.375 <= tab_position < 0.5:
                    candidate_tab = RuneLiteTab.INVENTORY
                elif 0.5 <= tab_position < 0.625:
                    candidate_tab = RuneLiteTab.EQUIPMENT
                elif 0.625 <= tab_position < 0.75:
                    candidate_tab = RuneLiteTab.PRAYER
                elif 0.75 <= tab_position < 0.875:
                    candidate_tab = RuneLiteTab.MAGIC
                else:
                    candidate_tab = RuneLiteTab.CLAN
                
                # Calculate confidence based on area and brightness
                tab_roi = tab_region[y:y+h, x:x+w]
                brightness = np.mean(tab_roi)
                confidence = min(1.0, (area / 500.0) * (brightness / 255.0))
                
                if confidence > best_confidence:
                    best_confidence = confidence
                    best_tab = candidate_tab
        
        logger.debug(f"ðŸŽ¯ Tab detection: {best_tab.value} (confidence: {best_confidence:.2f})")
        return {'tab': best_tab, 'confidence': best_confidence}
    
    def _detect_orbs_with_ml(self, screenshot: np.ndarray) -> Dict[str, Any]:
        """
        ðŸ”® ML-POWERED ORB DETECTION using color clustering
        """
        orbs = {}
        h, w = screenshot.shape[:2]
        
        # Orb positions in game viewport (right side)
        orb_positions = {
            'health': (0.73, 0.05, 0.08, 0.08),
            'prayer': (0.73, 0.15, 0.08, 0.08),
            'energy': (0.73, 0.25, 0.08, 0.08),
            'special': (0.73, 0.35, 0.08, 0.08)
        }
        
        total_confidence = 0.0
        detected_count = 0
        
        for orb_name, (x_ratio, y_ratio, w_ratio, h_ratio) in orb_positions.items():
            # Extract orb region
            orb_x = int(w * x_ratio)
            orb_y = int(h * y_ratio)
            orb_w = int(w * w_ratio)
            orb_h = int(h * h_ratio)
            
            orb_region = screenshot[orb_y:orb_y+orb_h, orb_x:orb_x+orb_w]
            
            if orb_region.size == 0:
                continue
            
            # ðŸ§  MACHINE LEARNING: Color clustering analysis
            orb_analysis = self._analyze_orb_with_ml(orb_region, orb_name)
            
            if orb_analysis['confidence'] > 0.3:
                orbs[orb_name] = orb_analysis
                total_confidence += orb_analysis['confidence']
                detected_count += 1
        
        avg_confidence = total_confidence / max(1, detected_count)
        
        logger.debug(f"ðŸ”® Orb detection: {detected_count} orbs, avg confidence: {avg_confidence:.2f}")
        return {'orbs': orbs, 'confidence': avg_confidence}
    
    def _analyze_orb_with_ml(self, orb_region: np.ndarray, orb_type: str) -> Dict[str, Any]:
        """
        ðŸ§  MACHINE LEARNING analysis of individual orb
        """
        try:
            # Basic analysis if ML libraries not available
            if not HAS_SKLEARN:
                brightness = np.mean(orb_region)
                return {
                    'state': 'detected',
                    'confidence': min(1.0, brightness / 255.0),
                    'method': 'basic_brightness'
                }
            
            # ðŸ”¥ ADVANCED ML ANALYSIS
            
            # Reshape for clustering
            pixels = orb_region.reshape(-1, 3)
            if len(pixels) < 10:
                return {'state': 'unknown', 'confidence': 0.0}
            
            # K-means clustering to find dominant colors
            n_clusters = min(5, len(pixels))
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            kmeans.fit(pixels)
            
            # Analyze cluster centers (dominant colors)
            cluster_centers = kmeans.cluster_centers_
            cluster_sizes = np.bincount(kmeans.labels_)
            
            # Find most prominent color
            largest_cluster_idx = np.argmax(cluster_sizes)
            dominant_color = cluster_centers[largest_cluster_idx]
            
            # Classify orb state based on color
            state = self._classify_orb_state_advanced(dominant_color, orb_type)
            
            # Calculate confidence
            color_variance = np.std(orb_region)
            cluster_dominance = cluster_sizes[largest_cluster_idx] / len(pixels)
            
            confidence = min(1.0, cluster_dominance * (color_variance / 50.0))
            
            return {
                'state': state,
                'confidence': confidence,
                'dominant_color': dominant_color.tolist(),
                'color_variance': float(color_variance),
                'method': 'ml_clustering'
            }
            
        except Exception as e:
            logger.debug(f"ML orb analysis failed for {orb_type}: {e}")
            return {'state': 'error', 'confidence': 0.0}
    
    def _classify_orb_state_advanced(self, color: np.ndarray, orb_type: str) -> str:
        """
        ðŸŽ¯ ADVANCED color-based state classification
        """
        b, g, r = color
        
        # Convert to HSV for better color analysis
        bgr_pixel = np.uint8([[[b, g, r]]])
        hsv_pixel = cv2.cvtColor(bgr_pixel, cv2.COLOR_BGR2HSV)[0, 0]
        h, s, v = hsv_pixel
        
        if orb_type == 'health':
            if h < 30 or h > 150:  # Red hues
                return 'low_health' if v > 100 else 'critical_health'
            elif 30 <= h <= 90:  # Green hues
                return 'high_health'
            else:
                return 'medium_health'
        
        elif orb_type == 'prayer':
            if 90 <= h <= 130:  # Blue hues
                return 'high_prayer' if v > 100 else 'medium_prayer'
            else:
                return 'low_prayer'
        
        elif orb_type == 'energy':
            if 15 <= h <= 35:  # Yellow hues
                return 'high_energy' if v > 150 else 'medium_energy'
            else:
                return 'low_energy'
        
        elif orb_type == 'special':
            if 80 <= h <= 100:  # Cyan hues
                return 'special_ready'
            else:
                return 'special_charging'
        
        return 'unknown'
    
    def _detect_inventory_with_contours(self, screenshot: np.ndarray) -> Dict[str, Any]:
        """
        ðŸ“¦ ADVANCED INVENTORY DETECTION using contour analysis
        """
        items = []
        h, w = screenshot.shape[:2]
        
        # Extract inventory region
        inv_x = int(w * 0.55)
        inv_y = int(h * 0.35)
        inv_w = int(w * 0.25)
        inv_h = int(h * 0.30)
        
        inv_region = screenshot[inv_y:inv_y+inv_h, inv_x:inv_x+inv_w]
        
        if inv_region.size == 0:
            return {'items': [], 'confidence': 0.0}
        
        # ðŸ”¥ ADVANCED: Edge detection for item boundaries
        gray_inv = cv2.cvtColor(inv_region, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray_inv, 30, 100)
        
        # Morphological operations to clean up edges
        kernel = np.ones((3, 3), np.uint8)
        edges_cleaned = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)
        
        # Find contours (potential items)
        contours, _ = cv2.findContours(edges_cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for i, contour in enumerate(contours):
            area = cv2.contourArea(contour)
            
            # Filter for item-sized areas
            if 50 < area < 1500:
                x, y, w, h = cv2.boundingRect(contour)
                
                # Extract item region for analysis
                item_roi = inv_region[y:y+h, x:x+w]
                
                # Analyze item characteristics
                color_variance = np.std(item_roi)
                brightness = np.mean(item_roi)
                
                # Item detection criteria
                if color_variance > 10 and brightness > 30:
                    confidence = min(1.0, (color_variance / 30.0) * (area / 300.0))
                    
                    items.append({
                        'slot': i,
                        'name': f'Advanced_Item_{i}',
                        'confidence': confidence,
                        'area': int(area),
                        'color_variance': float(color_variance),
                        'position': (x + w//2, y + h//2),
                        'method': 'contour_analysis'
                    })
        
        avg_confidence = np.mean([item['confidence'] for item in items]) if items else 0.0
        
        logger.debug(f"ðŸ“¦ Inventory: {len(items)} items detected (confidence: {avg_confidence:.2f})")
        return {'items': items, 'confidence': avg_confidence}
    
    def _detect_chat_with_text_analysis(self, screenshot: np.ndarray) -> Dict[str, Any]:
        """
        ðŸ’¬ ADVANCED CHAT DETECTION using text region analysis
        """
        messages = []
        h, w = screenshot.shape[:2]
        
        # Extract chat region
        chat_x = int(w * 0.01)
        chat_y = int(h * 0.75)
        chat_w = int(w * 0.78)
        chat_h = int(h * 0.24)
        
        chat_region = screenshot[chat_y:chat_y+chat_h, chat_x:chat_x+chat_w]
        
        if chat_region.size == 0:
            return {'messages': [], 'confidence': 0.0}
        
        # ðŸ”¥ ADVANCED: Multiple thresholding for text detection
        gray_chat = cv2.cvtColor(chat_region, cv2.COLOR_BGR2GRAY)
        
        # Method 1: OTSU thresholding
        _, thresh_otsu = cv2.threshold(gray_chat, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # Method 2: Adaptive thresholding
        thresh_adaptive = cv2.adaptiveThreshold(
            gray_chat, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
        )
        
        # Method 3: Color-based text detection (white text)
        _, thresh_white = cv2.threshold(gray_chat, 200, 255, cv2.THRESH_BINARY)
        
        # Combine all methods
        combined_thresh = cv2.bitwise_or(thresh_otsu, cv2.bitwise_or(thresh_adaptive, thresh_white))
        
        # Find text contours
        contours, _ = cv2.findContours(combined_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            
            # Filter for text-like dimensions
            if 20 < w < 500 and 8 < h < 35:
                aspect_ratio = w / h
                
                # Text typically has high aspect ratio
                if 2 < aspect_ratio < 25:
                    confidence = min(1.0, aspect_ratio / 15.0)
                    
                    messages.append({
                        'type': 'advanced_chat',
                        'position': (x + w//2, y + h//2),
                        'size': (w, h),
                        'confidence': confidence,
                        'aspect_ratio': aspect_ratio,
                        'method': 'multi_threshold'
                    })
        
        avg_confidence = np.mean([msg['confidence'] for msg in messages]) if messages else 0.0
        
        logger.debug(f"ðŸ’¬ Chat: {len(messages)} text areas (confidence: {avg_confidence:.2f})")
        return {'messages': messages, 'confidence': avg_confidence}
    
    def _empty_results(self) -> Dict[str, Any]:
        """Empty results structure"""
        return {
            'active_tab': RuneLiteTab.UNKNOWN,
            'orbs': {},
            'inventory_items': [],
            'chat_messages': [],
            'confidence_scores': {},
            'detection_method': 'advanced_cv'
        }

# Global advanced vision instance
advanced_runelite_vision = AdvancedRuneLiteVision() 